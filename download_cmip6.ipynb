{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess CMIP6 data\n",
    "Script for downloading and saving CMIP6 files, with ability to subset by time and space. CMIP6 data is lazily loaded directly from the cloud, using the Pangeo - Google Cloud Public Dataset Program collaboration (more info [here](https://medium.com/pangeo/cmip6-in-the-cloud-five-ways-96b177abe396)).\n",
    "\n",
    "For each model, files are placed in a subdirectory of the `raw_data_dir` set in the `dwnld_config.py` file `[raw_data_dir]/[model_name]/`. If this subdirectory doesn't yet exist, it is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cftime\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from operator import itemgetter # For list subsetting but this is idiotic\n",
    "import intake\n",
    "import gcsfs\n",
    "import os\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config file \n",
    "import dwnld_config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set whether to regrid 360-day calendars to 365-day calendars\n",
    "# (probably don't do this while saving files. Only do this in \n",
    "# processing code that doesn't affect the original file)\n",
    "regrid_360 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CMIP6 data parameters: each of these dict keys \n",
    "# is a column in the dataframe that gives the link \n",
    "# of each of the relevant files. You'll hopefully not \n",
    "# need to go more specific than this. Files defined by\n",
    "# each dict are processed separately. \n",
    "data_params_all =[{'experiment_id':'historical','table_id':'day','variable_id':'pr','member_id':'r1i1p1f1'},\n",
    "               {'experiment_id':'ssp370','table_id':'day','variable_id':'pr','member_id':'r1i1p1f1'},\n",
    "               {'experiment_id':'ssp585','table_id':'day','variable_id':'pr','member_id':'r1i1p1f1'},\n",
    "               {'experiment_id':'historical','table_id':'day','variable_id':'tas','member_id':'r1i1p1f1'},\n",
    "               {'experiment_id':'ssp370','table_id':'day','variable_id':'pr','member_id':'r1i1p1f1'},\n",
    "               {'experiment_id':'ssp585','table_id':'day','variable_id':'tas','member_id':'r1i1p1f1'}] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for spatiotemporal subsetting\n",
    "subset_params_all = [{'lat':[-38,40],'lon':[-20,54],\n",
    "                  'time':{'historical':['1979-01-01','2014-12-31'],'ssp585':['2015-01-01','2100-12-31'],\n",
    "                          'ssp370':['2015-01-01','2100-12-31']}, # make sure to specify the experiment id for each time range\n",
    "                 'fn_suffix':'_Africa', # added to end of filename when saving\n",
    "                 'lon_range':180, # 180 or 360 - do you want your output file to count lon -180:180 or 0:360?\n",
    "                 'lon_origin':-180}] # set origin (first lon value) of pre-processed grid. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fix_lons` aux function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lons(ds,subset_params):\n",
    "    \"\"\"\n",
    "    This function fixes a few issues that show up when dealing with \n",
    "    longitude values. \n",
    "    \n",
    "    Input: an xarray dataset, with a longitude dimension called \"lon\"\n",
    "    \n",
    "    Changes: \n",
    "    - The dataset is re-indexed to -180:180 or 0:360 longitude format, \n",
    "      depending on the subset_params['lon_range'] parameter\n",
    "    - the origin (the first longitude value) is changed to the closest \n",
    "      lon value to subset_params['lon_origin'], if using a 0:360 range. \n",
    "      In other words, the range becomes [lon_origin:360 0:lon_origin]. \n",
    "      This is to make sure the subsetting occurs in the 'right' direction, \n",
    "      with the longitude indices increasing consecutively (this is to ensure\n",
    "      that subsetting to, say, [45, 275] doesn't subset to [275, 45] or vice-\n",
    "      versa). Set lon_origin to a longitude value lower than your first subset \n",
    "      value.\n",
    "    \"\"\"\n",
    "    \n",
    "    if subset_params['lon_range']==180:\n",
    "        # Switch to -180:180 longitude if necessary\n",
    "        if any (ds.lon>180):\n",
    "            ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180))\n",
    "        # Change origin to half the world over, to allow for the \n",
    "        # longitude indexing to cross the prime meridian, but only\n",
    "        # if the first lon isn't around -180 (using 5deg as an approx\n",
    "        # biggest grid spacing). This is intended to move [0:180 -180:0]\n",
    "        # to [-180:0:180].\n",
    "        if ds.lon[0] > -175:\n",
    "            ds = ds.roll(lon=(ds.sizes['lon'] // 2),roll_coords=True)\n",
    "    elif subset_params['lon_range']==360:\n",
    "        # Switch to 0:360 longitude if necessary\n",
    "        ds = ds.assign_coords(lon = ds.lon % 360)\n",
    "        # Change origin to the lon_origin\n",
    "        ds = ds.roll(lon=-((ds.lon // subset_params['lon_origin'])==1).values.nonzero()[0][0],roll_coords=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the full query for all the datasets that will end up getting use in this \n",
    "# process - this is to create the master dataset, so to build up the 'model' and \n",
    "# 'experiment' dimension in the dataset with all the values that will end up used\n",
    "source_calls = np.zeros(len(data_params_all[0].keys()))\n",
    "\n",
    "for key in data_params_all[0].keys():\n",
    "    if len(np.unique([x[key] for x in data_params_all]))==1:\n",
    "        source_calls[list(data_params_all[0].keys()).index(key)] = 1\n",
    "\n",
    "# First get all the ones with the same value for each key \n",
    "subset_query = ' and '.join([k+\" == '\"+data_params_all[0][k]+\"'\" for k in itemgetter(*source_calls.nonzero()[0])(list(data_params_all[0].keys()))])\n",
    "\n",
    "# Now add all that are different between subset params - i.e. those that need an OR statement\n",
    "# These have to be in two statements, because if there's only one OR'ed statement, then the \n",
    "# for k in statement goes through the letters instead of the keys. \n",
    "if len((source_calls-1).nonzero()[0])==1:\n",
    "    subset_query=subset_query+' and ('+') and ('.join([' or '.join([k+\" == '\"+data_params[k]+\"'\" for data_params in data_params_all]) \n",
    "              for k in [itemgetter(*(source_calls-1).nonzero()[0])(list(data_params_all[0].keys()))]])+')'\n",
    "elif len((source_calls-1).nonzero()[0])>1:\n",
    "    subset_query=subset_query+' and ('+') and ('.join([' or '.join([k+\" == '\"+data_params[k]+\"'\" for data_params in data_params_all]) \n",
    "              for k in itemgetter(*(source_calls-1).nonzero()[0])(list(data_params_all[0].keys()))])+')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access google cloud storage links\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only')\n",
    "# Get info about CMIP6 datasets\n",
    "cmip6_datasets = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "# Get subset based on the data params above (for all search parameters)\n",
    "cmip6_sub = cmip6_datasets.query(subset_query)\n",
    "\n",
    "if len(cmip6_sub) == 0:\n",
    "    warnings.warn('Query unsuccessful, no files found! Check to make sure your table_id matches the domain - for example, SSTs are listed as \"Oday\" instead of \"day\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------ Process by variable and dataset in the subset ------\n",
    "overwrite=True\n",
    "for data_params in data_params_all:\n",
    "    # Get subset based on the data params above, now just for this one variable\n",
    "    cmip6_sub = cmip6_datasets.query(' and '.join([k+\" == '\"+data_params[k]+\"'\" for k in data_params.keys() if k is not 'other']))\n",
    "     \n",
    "    for url in tqdm(cmip6_sub.zstore.values):\n",
    "        # Set output filenames\n",
    "        output_fns = [None]*len(subset_params_all)\n",
    "        path_exists = [None]*len(subset_params_all)\n",
    "        for subset_params in subset_params_all:\n",
    "            output_fns[subset_params_all.index(subset_params)] = (cfg.lpaths['raw_data_dir']+url.split('/')[5]+'/'+\n",
    "                                                                 data_params['variable_id']+'_'+\n",
    "                                                                 data_params['table_id']+'_'+url.split('/')[5]+'_'+\n",
    "                                                                 data_params['experiment_id']+'_'+data_params['member_id']+'_'+\n",
    "                                                                 '-'.join([re.sub('-','',t) for t in subset_params['time'][data_params['experiment_id']]])+\n",
    "                                                                 subset_params['fn_suffix']+'.nc')\n",
    "            \n",
    "            if 'other' in data_params.keys(): \n",
    "                if 'plev_subset' in data_params['other'].keys():\n",
    "                    output_fns[subset_params_all.index(subset_params)] = re.sub(data_params['variable_id'],\n",
    "                                                                            data_params['other']['plev_subset']['outputfn'],\n",
    "                                                                           output_fns[subset_params_all.index(subset_params)])\n",
    "            \n",
    "            \n",
    "            \n",
    "            path_exists[subset_params_all.index(subset_params)] = os.path.exists(output_fns[subset_params_all.index(subset_params)])\n",
    "        \n",
    "        if (not overwrite) & all(path_exists):\n",
    "            warnings.warn('All files already created for '+data_params['variable_id']+' '+\n",
    "                                                                 data_params['table_id']+' '+url.split('/')[5]+' '+\n",
    "                                                                 data_params['experiment_id']+' '+data_params['member_id']+', skipped.')\n",
    "            continue\n",
    "        elif any(path_exists):\n",
    "            if overwrite:\n",
    "                for subset_params in subset_params_all:\n",
    "                    if path_exists[subset_params_all.index(subset_params)]:\n",
    "                        os.remove(output_fns[subset_params_all.index(subset_params)])\n",
    "                        warnings.warn('All files already exist for '+data_params['variable_id']+' '+\n",
    "                                                                             data_params['table_id']+' '+url.split('/')[5]+' '+\n",
    "                                                                             data_params['experiment_id']+' '+data_params['member_id']+\n",
    "                                      ', because OVERWRITE=TRUE theses files have been deleted.')\n",
    "        \n",
    "        \n",
    "        # Open dataset\n",
    "        ds = xr.open_zarr(fs.get_mapper(url),consolidated=True)\n",
    "\n",
    "        # Rename to lat / lon (let's hope there's no \n",
    "        # Latitude / latitude_1 / etc. in this dataset)\n",
    "        try:\n",
    "            ds = ds.rename({'longitude':'lon','latitude':'lat'})\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        # same with 'nav_lat' and 'nav_lon' ???\n",
    "        try:\n",
    "            ds = ds.rename({'nav_lon':'lon','nav_lat':'lat'})\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # If precip, kg/m^2/s, switch to mm/day\n",
    "        #if data_params['variable_id']=='pr':\n",
    "        #    ds[data_params['variable_id']] = ds[data_params['variable_id']]*60*60*24\n",
    "\n",
    "        # Fix coordinate doubling (this was an issue in NorCPM1, \n",
    "        # where thankfully the values of the variables were nans,\n",
    "        # though I still don't know how this happened - some lat\n",
    "        # values were doubled within floating point errors)\n",
    "        if 'lat' in ds[data_params['variable_id']].dims:\n",
    "            if len(np.unique(np.round(ds.lat.values,10))) != ds.dims['lat']:\n",
    "                ds = ds.isel(lat=(~np.isnan(ds.isel(lon=1,time=1)[data_params['variable_id']].values)).nonzero()[0],drop=True)\n",
    "                warnings.warn('Model '+ds.source_id+' has duplicate lat values; attempting to compensate by dropping lat values that are nan in the main variable in the first timestep')\n",
    "            if len(np.unique(np.round(ds.lon.values,10))) != ds.dims['lon']:\n",
    "                ds = ds.isel(lon=(~np.isnan(ds.isel(lat=1,time=1)[data_params['variable_id']].values)).nonzero()[0],drop=True)\n",
    "                warnings.warn('Model '+ds.source_id+' has duplicate lon values; attempting to compensate by dropping lon values that are nan in the main variable in the first timestep')\n",
    "\n",
    "        # Sort by time, if not sorted (this happened with\n",
    "        # a model; keeping a warning, cuz this seems weird)\n",
    "        if (ds.time.values != np.sort(ds.time)).any():\n",
    "            warnings.warn('Model '+ds.source_id+' has an unsorted time dimension.')\n",
    "            ds = ds.sortby('time')\n",
    "            \n",
    "        # If 360-day calendar, regrid to 365-day calendar\n",
    "        if regrid_360:\n",
    "            if ds.dims['dayofyear'] == 360:\n",
    "                # Have to put in the compute() because these \n",
    "                # are by default dask arrays, chunked along\n",
    "                # the time dimension, and can't interpolate\n",
    "                # across dask chunks... \n",
    "                ds = ds.compute().interp(dayofyear=(np.arange(1,366)/365)*360)\n",
    "                # And reset it to 1:365 indexing on day of year\n",
    "                ds['dayofyear'] = np.arange(1,366)\n",
    "                # Throw in a warning, too, why not\n",
    "                warnings.warn('Model '+ds.source_id+' has a 360-day calendar; daily values were interpolated to a 365-day calendar')\n",
    "\n",
    "        # Now, save by the subsets desired in subset_params_all above\n",
    "        for subset_params in subset_params_all:\n",
    "            # Make sure this file hasn't already been processed\n",
    "            if (not overwrite) & path_exists[subset_params_all.index(subset_params)]:\n",
    "                warnings.warn(output_fns[subset_params_all.index(subset_params)]+' already exists; skipped.')\n",
    "                continue\n",
    "            \n",
    "            # Make sure the target directory exists\n",
    "            if not os.path.exists(cfg.lpaths['raw_data_dir']+url.split('/')[5]+'/'):\n",
    "                os.mkdir(cfg.lpaths['raw_data_dir']+url.split('/')[5]+'/')\n",
    "                warnings.warn('Directory '+cfg.lpaths['raw_data_dir']+url.split('/')[5]+'/'+' created!')\n",
    "         \n",
    "            # Fix longitude (by setting it to either [-180:180] \n",
    "            # or [0:360] as determined by subset_params, and \n",
    "            # to roll them so the correct range is consecutive \n",
    "            # in lon (so if you're looking at the Equatorial \n",
    "            # Pacific, make it 0:360, with the first lon value\n",
    "            # at 45E). \n",
    "            if 'lat' in ds[data_params['variable_id']].dims:\n",
    "                ds_tmp = fix_lons(ds,subset_params)\n",
    "                # Now, cutoff the values below the 'lon_origin', \n",
    "                # because slice doesn't work if the indices aren't\n",
    "                # montonically increasing (and the above changes it\n",
    "                # to [lon_origin:360 0:lon_origin]\n",
    "                if np.abs(ds_tmp.lon[0]-subset_params['lon_origin'])>5:\n",
    "                    ds_tmp = ds_tmp.isel(lon=np.arange(0,(ds_tmp.lon // (subset_params['lon_origin']) == 0).values.nonzero()[0][0]))\n",
    "            else:\n",
    "                ds_tmp = ds\n",
    "                warnings.warn('fix_lons did not work because of the multi-dimensional index')\n",
    "\n",
    "            # Subset by time as set in subset_params\n",
    "            if (ds.time.max().dt.day==30) | (type(ds.time.values[0]) == cftime._cftime.Datetime360Day): \n",
    "                # (If it's a 360-day calendar, then subsetting to \"12-31\"\n",
    "                # will throw an error; this switches that call to \"12-30\")\n",
    "                # Also checking explicitly for 360day calendar; some monthly \n",
    "                # data is still shown as 360-day even when it's monthly, and will\n",
    "                # fail on date ranges with date 31 in a month\n",
    "                ds_tmp = (ds_tmp.sel(time=slice(subset_params['time'][data_params['experiment_id']][0],\n",
    "                                        re.sub('-31','-30',subset_params['time'][data_params['experiment_id']][1]))))\n",
    "            else:\n",
    "                ds_tmp = (ds_tmp.sel(time=slice(*subset_params['time'][data_params['experiment_id']])))\n",
    "            \n",
    "           # Subset by space as set in subset_params\n",
    "            if not 'lat' in ds[data_params['variable_id']].dims:\n",
    "                ds_tmp = ds_tmp.where((ds_tmp.lat >= subset_params['lat'][0]) & (ds_tmp.lat <= subset_params['lat'][1]) &\n",
    "                 (ds_tmp.lon >= subset_params['lon'][0]) & (ds_tmp.lon <= subset_params['lon'][1]),drop=True)\n",
    "            else:\n",
    "                ds_tmp = (ds_tmp.sel(lat=slice(*subset_params['lat']),\n",
    "                                     lon=slice(*subset_params['lon'])))\n",
    "                \n",
    "            # If subsetting by pressure level...\n",
    "            if 'other' in data_params.keys():\n",
    "                if 'plev_subset' in data_params['other'].keys:\n",
    "                    # Have to use np.allclose for floating point errors\n",
    "                    try:\n",
    "                        ds_tmp = ds_tmp.isel(plev=np.where([np.allclose(p,data_params['other']['plev_subset']['plev']) for p in ds_tmp.plev])[0][0])\n",
    "                        ds_tmp = ds_tmp.rename({data_params['variable_id']:data_params['other']['plev_subset']['outputfn']})\n",
    "                    except KeyError:\n",
    "                        print('The pressure levels: ')\n",
    "                        print(ds_tmp.plev.values)\n",
    "                        print(' do not contain '+str(data_params['other']['plev_subset']['plev'])+'; skipping.')\n",
    "                        del ds_tmp\n",
    "                        continue\n",
    "\n",
    "            # Save as NetCDF file\n",
    "            ds_tmp.to_netcdf(output_fns[subset_params_all.index(subset_params)])\n",
    "\n",
    "            # Status update\n",
    "            print(output_fns[subset_params_all.index(subset_params)]+' processed!')\n",
    "        \n",
    "        del ds, ds_tmp, subset_params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
